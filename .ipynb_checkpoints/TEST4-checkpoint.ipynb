{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8436b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dhanu\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dhanu\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dhanu\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dhanu\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Mean Squared Error: 11.420679834444574\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True, parser='auto')\n",
    "\n",
    "# Extract features and target variable\n",
    "X = boston.data.to_numpy()  # Features\n",
    "y = boston.target.to_numpy()  # Target variable (house prices)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit on training data and transform it\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform test data\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train_scaled.shape[1],)))  # Input layer specifying the input shape\n",
    "model.add(Dense(64, activation='relu'))  # Hidden layer with 64 neurons\n",
    "model.add(Dense(32, activation='relu'))  # Hidden layer with 32 neurons\n",
    "model.add(Dense(1))  # Output layer with 1 neuron (no activation for regression)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')  # Using MSE loss and Adam optimizer\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0)  # Train for 100 epochs\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate mean squared error (MSE) as the evaluation metric\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b2f01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO network loaded successfully\n",
      "Classes loaded: ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Specify the paths to the YOLO configuration and weights files\n",
    "config_path = \"C:/Users/dhanu/Downloads/yolov3.cfg\"  # Replace with the correct path to your yolov3.cfg\n",
    "weights_path = \"C:/Users/dhanu/Downloads/yolov3.weights\"  # Replace with the correct path to your yolov3.weights\n",
    "names_path = \"C:/Users/dhanu/Downloads/coco.names\"  # Replace with the correct path to your coco.names\n",
    "\n",
    "# Load YOLO network\n",
    "net = cv2.dnn.readNet(weights_path, config_path)\n",
    "\n",
    "# Load class names\n",
    "classes = []\n",
    "print(\"YOLO network loaded successfully\")\n",
    "print(\"Classes loaded:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8559777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930f3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
